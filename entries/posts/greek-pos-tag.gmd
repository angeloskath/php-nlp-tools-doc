---
[metadata]
created = Sat, Jun 29 2013 10:13 UTC

title = Developing a Greek part of speech tagger
project = Greek POS Tagger
projectpath = /blog/category/greek-pos-tagger

type = post
[options]
layout=layouts/page.php
---

Within a recent attempt of mine to develop tools for natural language processing in Greek I decided to implement
the state of the art in Greek POS tagging (the paper reporting the best accuracy that I could find) and offer the
result in a way that it will be easy to use in one's projects as a subsystem.

The tagger is based on the work of [Evangelia Koleli](http://uk.linkedin.com/pub/evangelia-koleli/48/21a/931) and
her [BSc thesis](http://nlp.cs.aueb.gr/theses/ekoleli_final_report.pdf) (the paper is in Greek) in 2011. She was
kind enough to provide me with the dataset she used. Evangelia's work also provides an API for using the tagger in
your own projects (in Java), I am aiming at even simpler integration (and especially web projects hence php).

In this series of posts I will be documenting the development of the tagger.

### Problem statement ###

> In corpus linguistics, part-of-speech tagging (POS tagging or POST), also called grammatical tagging
> or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding
> to a particular part of speech, based on both its definition, as well as its context.
>
> <cite> Wikipedia </cite>

In this project I will be using only the small set of parts of speech because I believe it is a good tradeoff
between information and model simplicity. The small set of categories as defined by Evangelia Koleli is the
following:

* verb
* noun
* adjective
* adverb
* article
* pronoun
* numeral
* preposition
* particle
* conjuction
* punctuation
* other

### The approach ###

POS Tagging is a sequence tagging problem. The algorithms used aim at finding the most likely sequence of
tags that could correspond to the observed sequence, the words. It is also, usually, dealt with on a per
sentence basis.

For this tagger the approach is actually quite simpler and straightforward. We deal with the problem as a
simple classification problem. We can use the complete word sequence for feature creation but
none of the tag sequence (ex.: we cannot use the knowledge that the previous word is a noun but we can use the
previous word).

### Baseline ###

Before we start discussing the tagger implementation I would like to present a baseline parser. The baseline
parser simply tags each word with the most common tag this word had in the training set. If it has never seen
the word before then it tags it with the most common tag (usually noun). Actually, adding a set of targeted
transformation rules to the above baseline classifier results in the simple and effective [Brill Tagger](http://en.wikipedia.org/wiki/Brill_tagger).

The Baseline parser usually has an acuraccy of approximately 90% which is huge for a baseline, compared to
other classification tasks, but it makes intuitive sense for this task since many words can only be tagged with
one part of speech.

~~~ php
class Baseline implements Classifier
{
	protected $map;
	protected $classmap;

	public function __construct(TrainingSet $training) {
		$entry = array_fill_keys($training->getClassSet(), 0);
		$this->classmap = $entry;
		foreach ($training as $class=>$d) {
			list($w) = $d->getDocumentData();
			$w = mb_strtolower($w,"utf-8");
			if (!isset($this->map[$w]))
				$this->map[$w] = $entry;
			$this->map[$w][$class]++;
			$this->classmap[$class]++;
		}
		foreach ($this->map as &$occ) {
			arsort($occ);
		}
		arsort($this->classmap);
	}

	public function classify(array $classes, Document $d) {
		list($w) = $d->getDocumentData();
		$w = mb_strtolower($w,"utf-8");

		if (!isset($this->map[$w]))
			return key($this->classmap);

		return key($this->map[$w]);
	}
}
~~~

This Baseline tagger learned from the training set has an accuracy of **82.62%** on the test set. Yet this tagger is
a very poor tagger and this can be easily illustrated if one sees the confusion matrix or simply the per tag
precision and recall shown in the table below.

<table class="small-table">
	<tr>
		<td style="border: 0;"></td>
		<td>verb       </td>
		<td>noun       </td>
		<td>adjective  </td>
		<td>adverb     </td>
		<td>article    </td>
		<td>pronoun    </td>
		<td>numeral    </td>
		<td>preposition</td>
		<td>particle   </td>
		<td>conjuction </td>
		<td>punctuation</td>
		<td>other      </td>
	</tr>
	<tr>
		<td>Precision</td>
		<td class="text-right">100.0    </td>
		<td class="text-right">60.65    </td>
		<td class="text-right">91.03    </td>
		<td class="text-right">96.21    </td>
		<td class="text-right">94.80    </td>
		<td class="text-right">92.46    </td>
		<td class="text-right">96.95    </td>
		<td class="text-right">98.11    </td>
		<td class="text-right">98.92    </td>
		<td class="text-right">99.78    </td>
		<td class="text-right">100.0    </td>
		<td class="text-right">100.0    </td>
	</tr>
	<tr>
		<td>Recall</td>
		<td class="text-right">49.47 </td>
		<td class="text-right">98.90 </td>
		<td class="text-right">41.90 </td>
		<td class="text-right">85.23 </td>
		<td class="text-right">98.55 </td>
		<td class="text-right">73.50 </td>
		<td class="text-right">72.94 </td>
		<td class="text-right">99.18 </td>
		<td class="text-right">98.92 </td>
		<td class="text-right">98.09 </td>
		<td class="text-right">98.93 </td>
		<td class="text-right">26.82 </td>
	</tr>
</table>

From the table above one can see that more than half of the verbs have been tagged incorectly. By checking the
confusion matrix we see that they have been mostly tagged as nouns (most misclassifications are) which also explains
the really bad precision of the noun tag.

The above shows that accuracy or even precision, recall and F1 score are not good metrics of the quality
of a POS tagger. Using the confusion matrix one can understand how badly a misclassification would stand out to a
human evaluator. Classifying verbs as nouns definitely does not look good even with 90% accuracy.

In the next post we will be evaluating features using the much simpler Naive Bayes model.

